{
    "apiVersion": "v1",
    "items": [
        {
            "apiVersion": "v1",
            "data": {
                "ca.crt": "-----BEGIN CERTIFICATE-----\nMIIDBTCCAe2gAwIBAgIIdVZ4OXlm3howDQYJKoZIhvcNAQELBQAwFTETMBEGA1UE\nAxMKa3ViZXJuZXRlczAeFw0yNTEyMTgxODIxNTVaFw0zNTEyMTYxODI2NTVaMBUx\nEzARBgNVBAMTCmt1YmVybmV0ZXMwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEK\nAoIBAQDRmY/1mdgGLX5JJz4mQmbSxg5C9OxhaR4Z2iQnuZjfqv/Ad186OXBadxRw\neVgeye4P1iWvmjcqY9q3pbdjN2WdE4x1p/tXFN6A4onbZypXN8pkXSAqyf0+D/QX\n5UY7mRtMHjgrOqAQ/0Lr79r6lX8bKvwHq8oiVjpwcHHb08ouq99B7TCJZ5d6EYeQ\n3ZGdbt2Vo/JM2xGfoXm1NBP26L3EnuXCDHlrQp0ERAQM68KRiyCNNDynwjz/DLtX\nofZrG+p2pWYc0C4NvXH3B8/1PjiQ6kcc6noKoBFerfobuVLJmkW+8qhdInmkvRt6\nfVDpzklrxUNMp/rIMbdxxTQ5S4g9AgMBAAGjWTBXMA4GA1UdDwEB/wQEAwICpDAP\nBgNVHRMBAf8EBTADAQH/MB0GA1UdDgQWBBQBfYhN6j9YZrHNfGRZH+zhNzZh/TAV\nBgNVHREEDjAMggprdWJlcm5ldGVzMA0GCSqGSIb3DQEBCwUAA4IBAQBgnq6yWBQd\nC6Z/LReT/E39sJG03rzXz5NRoQBZMONAfvL+jlzRkqK+T70/jGZlu9mNNZFa7fck\npjoO6QXqBHli60feBMJeUTrK5az1V8rPgKpEt34Esy6NVc8S52RzSy78HtQejh2l\nXPNLA/BuGMOP1kuaizM7axYW4luqyankgHNXiKxxoK2qsc+mtELapmtSu1a3oaqx\nV6ZO/4/EoV25KORXWMQ4IZ7sK+dPbr6Mh/GejaZG2N2lTFAWA1a9C2ZootKg8VDO\nKFvVePd227BqD5153nQ7r8u3BckJj+9tfUn/JCTlDnDdx/YFqv64vZU7FWHIW7CU\nwGjni5x+bo3m\n-----END CERTIFICATE-----\n"
            },
            "kind": "ConfigMap",
            "metadata": {
                "annotations": {
                    "kubernetes.io/description": "Contains a CA bundle that can be used to verify the kube-apiserver when using internal endpoints such as the internal service IP or kubernetes.default.svc. No other usage is guaranteed across distributions of Kubernetes clusters."
                },
                "creationTimestamp": "2026-02-13T08:36:29Z",
                "name": "kube-root-ca.crt",
                "namespace": "elk",
                "resourceVersion": "10770339",
                "uid": "cde1e97a-3d7f-4d50-98f8-99fc9936c1c8"
            }
        },
        {
            "apiVersion": "v1",
            "data": {
                "sync.py": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nScript de sincronizaci√≥n MongoDB ‚Üí Elasticsearch\nVersi√≥n mejorada con l√≠mite de tama√±o de documentos y manejo robusto de errores\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nfrom datetime import datetime\nfrom decimal import Decimal\nfrom pymongo import MongoClient\nfrom bson import ObjectId, Decimal128\nimport requests\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nfrom urllib.parse import quote_plus\n\n# Configuraci√≥n desde variables de entorno\nMONGO_HOST = os.getenv('MONGO_HOST', 'mongo.mongo.svc.cluster.local')\nMONGO_PORT = int(os.getenv('MONGO_PORT', '27017'))\nMONGO_USERNAME = os.getenv('MONGO_USERNAME', 'root')\nMONGO_PASSWORD = os.getenv('MONGO_PASSWORD', '')\nMONGO_AUTH_DB = os.getenv('MONGO_AUTH_DB', 'admin')\n\nES_HOST = os.getenv('ELASTICSEARCH_HOST', 'elasticsearch.elk.svc.cluster.local')\nES_PORT = int(os.getenv('ELASTICSEARCH_PORT', '9200'))\nES_URL = f'http://{ES_HOST}:{ES_PORT}'\n\nBATCH_SIZE = int(os.getenv('BATCH_SIZE', '50'))\nMAX_DOCS_PER_COLLECTION = int(os.getenv('MAX_DOCS_PER_COLLECTION', '10000'))\n\n# NUEVO: L√≠mites de tama√±o optimizados\nMAX_DOC_SIZE_BYTES = int(os.getenv('MAX_DOC_SIZE_BYTES', '524288'))  # 512KB por documento\nCONTENT_FIELD_MAX_SIZE = int(os.getenv('CONTENT_FIELD_MAX_SIZE', '102400'))  # 100KB para campos de texto\nLARGE_FIELD_THRESHOLD = int(os.getenv('LARGE_FIELD_THRESHOLD', '51200'))  # 50KB umbral\n\n# Bases de datos del sistema que no se sincronizan\nSYSTEM_DBS = ['admin', 'config', 'local']\n\ndef print_header():\n    \"\"\"Imprime el encabezado del script\"\"\"\n    print(\"=\" * 60)\n    print(\"MongoDB ‚Üí Elasticsearch Sync (Mejorado v2)\")\n    print(\"=\" * 60)\n    print(f\"MongoDB: {MONGO_HOST}:{MONGO_PORT}\")\n    print(f\"Elasticsearch: {ES_URL}\")\n    print(f\"Batch size: {BATCH_SIZE}\")\n    print(f\"Max docs/collection: {MAX_DOCS_PER_COLLECTION}\")\n    print(f\"Max doc size: {MAX_DOC_SIZE_BYTES / 1024:.0f}KB\")\n    print(\"=\" * 60)\n    print()\n\ndef create_session():\n    \"\"\"Crea una sesi√≥n de requests con retry autom√°tico y pool de conexiones limitado\"\"\"\n    session = requests.Session()\n    \n    # Configurar retry autom√°tico\n    retry_strategy = Retry(\n        total=3,\n        backoff_factor=1,\n        status_forcelist=[429, 500, 502, 503, 504],\n    )\n    \n    adapter = HTTPAdapter(\n        max_retries=retry_strategy,\n        pool_connections=5,\n        pool_maxsize=10,\n        pool_block=True\n    )\n    \n    session.mount(\"http://\", adapter)\n    session.mount(\"https://\", adapter)\n    \n    return session\n\ndef connect_mongodb():\n    \"\"\"Conecta a MongoDB y retorna el cliente\"\"\"\n    try:\n        print(f\"‚Üí Conectando a MongoDB: {MONGO_HOST}:{MONGO_PORT}\")\n        \n        # Crear URI de conexi√≥n con URL encoding para username y password\n        if MONGO_USERNAME and MONGO_PASSWORD:\n            # URL encode username y password para manejar caracteres especiales\n            username_encoded = quote_plus(MONGO_USERNAME)\n            password_encoded = quote_plus(MONGO_PASSWORD)\n            mongo_uri = f\"mongodb://{username_encoded}:{password_encoded}@{MONGO_HOST}:{MONGO_PORT}/{MONGO_AUTH_DB}?authSource={MONGO_AUTH_DB}\"\n        else:\n            mongo_uri = f\"mongodb://{MONGO_HOST}:{MONGO_PORT}/\"\n        \n        # Conectar con configuraci√≥n optimizada\n        client = MongoClient(\n            mongo_uri,\n            serverSelectionTimeoutMS=10000,\n            connectTimeoutMS=10000,\n            maxPoolSize=10,\n            minPoolSize=1,\n            maxIdleTimeMS=30000\n        )\n        \n        # Verificar conexi√≥n\n        client.admin.command('ping')\n        server_info = client.server_info()\n        print(f\"‚úì Conectado a MongoDB {server_info['version']}\")\n        \n        return client\n    except Exception as e:\n        print(f\"‚úó Error al conectar a MongoDB: {e}\")\n        sys.exit(1)\n\ndef test_elasticsearch(session):\n    \"\"\"Verifica que Elasticsearch est√© accesible\"\"\"\n    try:\n        print(f\"‚Üí Conectando a Elasticsearch: {ES_URL}\")\n        response = session.get(ES_URL, timeout=10)\n        response.raise_for_status()\n        info = response.json()\n        print(f\"‚úì Conectado a Elasticsearch {info['version']['number']}\")\n        return True\n    except Exception as e:\n        print(f\"‚úó Error al conectar a Elasticsearch: {e}\")\n        print(f\"   URL intentada: {ES_URL}\")\n        sys.exit(1)\n\ndef get_index_name(db_name, collection_name):\n    \"\"\"Genera el nombre del √≠ndice en Elasticsearch\"\"\"\n    date_suffix = datetime.now().strftime('%Y.%m.%d')\n    index_name = f\"mongodb-{db_name}-{collection_name}-{date_suffix}\".lower()\n    index_name = index_name.replace('_', '-')\n    return index_name\n\ndef json_serializer(obj):\n    \"\"\"Serializador personalizado para tipos no est√°ndar de JSON\"\"\"\n    if isinstance(obj, ObjectId):\n        return str(obj)\n    elif isinstance(obj, datetime):\n        return obj.isoformat()\n    elif isinstance(obj, Decimal128):\n        return float(obj.to_decimal())\n    elif isinstance(obj, Decimal):\n        return float(obj)\n    elif isinstance(obj, bytes):\n        try:\n            return obj.decode('utf-8')\n        except:\n            return str(obj)\n    else:\n        return str(obj)\n\ndef truncate_large_fields(doc, max_size=MAX_DOC_SIZE_BYTES):\n    \"\"\"Trunca campos grandes en el documento con indexaci√≥n selectiva y metadata\"\"\"\n    try:\n        # Inicializar metadata de truncado\n        truncation_metadata = {\n            'fields_truncated': [],\n            'original_sizes': {},\n            'total_truncated': False\n        }\n        \n        # Convertir a JSON para medir tama√±o\n        json_str = json.dumps(doc, default=json_serializer, ensure_ascii=False)\n        current_size = len(json_str.encode('utf-8'))\n        \n        # Si est√° dentro del l√≠mite, no hacer nada\n        if current_size \u003c= max_size:\n            return doc, False\n        \n        # Identificar campos grandes\n        large_fields = []\n        for key, value in doc.items():\n            if key == '_id':\n                continue\n            try:\n                field_str = json.dumps(value, default=json_serializer, ensure_ascii=False)\n                field_size = len(field_str.encode('utf-8'))\n                if field_size \u003e LARGE_FIELD_THRESHOLD:  # 50KB umbral\n                    large_fields.append((key, field_size, value))\n            except:\n                continue\n        \n        # Truncar campos grandes con estrategia selectiva\n        truncated = False\n        for field_name, field_size, original_value in sorted(large_fields, key=lambda x: x[1], reverse=True):\n            # Guardar tama√±o original\n            truncation_metadata['original_sizes'][field_name] = field_size\n            \n            if isinstance(doc[field_name], str):\n                # Truncar strings largos a 100KB\n                max_chars = CONTENT_FIELD_MAX_SIZE // 4  # ~25K caracteres para UTF-8\n                if len(doc[field_name]) \u003e max_chars:\n                    doc[field_name] = doc[field_name][:max_chars] + \"\\n\\n[... CONTENT TRUNCATED ...]\\n\\nOriginal size: \" + str(field_size) + \" bytes\"\n                    truncated = True\n                    truncation_metadata['fields_truncated'].append(field_name)\n                    \n            elif isinstance(doc[field_name], list):\n                # Para listas, intentar reducir elementos\n                if field_size \u003e CONTENT_FIELD_MAX_SIZE:\n                    # Calcular cu√°ntos elementos mantener\n                    avg_item_size = field_size / len(doc[field_name]) if len(doc[field_name]) \u003e 0 else field_size\n                    keep_items = max(1, int(CONTENT_FIELD_MAX_SIZE / avg_item_size))\n                    doc[field_name] = doc[field_name][:keep_items]\n                    doc[field_name].append({\n                        \"_truncation_info\": f\"List truncated from {len(original_value)} to {keep_items} items\",\n                        \"original_size_bytes\": field_size\n                    })\n                    truncated = True\n                    truncation_metadata['fields_truncated'].append(field_name)\n                    \n            elif isinstance(doc[field_name], dict):\n                # Para diccionarios grandes, mantener estructura pero truncar valores\n                if field_size \u003e CONTENT_FIELD_MAX_SIZE:\n                    # Convertir a string resumido\n                    doc[field_name] = {\n                        \"_truncation_info\": \"Large object truncated\",\n                        \"original_size_bytes\": field_size,\n                        \"keys_count\": len(doc[field_name]),\n                        \"sample_keys\": list(doc[field_name].keys())[:10]\n                    }\n                    truncated = True\n                    truncation_metadata['fields_truncated'].append(field_name)\n            \n            # Verificar si ya est√° dentro del l√≠mite\n            json_str = json.dumps(doc, default=json_serializer, ensure_ascii=False)\n            current_size = len(json_str.encode('utf-8'))\n            if current_size \u003c= max_size:\n                break\n        \n        # A√±adir metadata de truncado al documento\n        if truncated:\n            truncation_metadata['total_truncated'] = True\n            doc['_truncation_metadata'] = truncation_metadata\n        \n        return doc, truncated\n    except Exception as e:\n        print(f\"      ‚ö† Error al truncar documento: {e}\")\n        return doc, False\n\ndef convert_doc(doc, db_name=None, collection_name=None):\n    \"\"\"Convierte un documento de MongoDB a formato JSON serializable\"\"\"\n    try:\n        # Convertir ObjectId a string\n        if '_id' in doc:\n            doc['_id'] = str(doc['_id'])\n        \n        # A√±adir timestamp de indexaci√≥n\n        doc['@timestamp'] = datetime.utcnow().isoformat() + 'Z'\n        \n        # A√±adir metadata de origen\n        if db_name:\n            doc['mongodb_database'] = db_name\n        if collection_name:\n            doc['mongodb_collection'] = collection_name\n        if db_name and collection_name:\n            doc['mongodb_namespace'] = f\"{db_name}.{collection_name}\"\n        \n        # Verificar y truncar si es necesario\n        doc, was_truncated = truncate_large_fields(doc)\n        \n        if was_truncated:\n            print(f\"      ‚ö† Documento {doc['_id']} truncado por tama√±o\")\n        \n        # Intentar serializar para detectar problemas\n        json.dumps(doc, default=json_serializer)\n        return doc\n    except Exception as e:\n        print(f\"      ‚ö† Error al convertir documento: {e}\")\n        return None\n\ndef bulk_index_documents(session, index_name, documents, retry_count=0):\n    \"\"\"Indexa documentos en Elasticsearch usando Bulk API con retry autom√°tico\"\"\"\n    if not documents:\n        return 0, 0\n    \n    # L√≠mite de reintentos\n    if retry_count \u003e 3:\n        print(f\"      ‚úó M√°ximo de reintentos alcanzado\")\n        return 0, len(documents)\n    \n    # Preparar el payload para Bulk API\n    lines = []\n    for doc in documents:\n        if doc is None:\n            continue\n        \n        try:\n            # Extraer el _id y eliminarlo del documento\n            doc_id = doc.get('_id', '')\n            \n            # Crear una copia del documento SIN el campo _id\n            doc_without_id = {k: v for k, v in doc.items() if k != '_id'}\n            \n            # Acci√≥n de indexaci√≥n\n            action = {\"index\": {\"_index\": index_name, \"_id\": doc_id}}\n            action_line = json.dumps(action, default=json_serializer)\n            \n            # Documento\n            doc_line = json.dumps(doc_without_id, default=json_serializer, ensure_ascii=False)\n            \n            lines.append(action_line)\n            lines.append(doc_line)\n        except Exception as e:\n            print(f\"      ‚ö† Error al serializar documento {doc.get('_id', 'unknown')}: {e}\")\n            continue\n    \n    if not lines:\n        return 0, 0\n    \n    # Unir con newlines\n    bulk_payload = '\\n'.join(lines) + '\\n'\n    \n    # Enviar a Elasticsearch\n    url = f'{ES_URL}/_bulk'\n    headers = {'Content-Type': 'application/x-ndjson'}\n    \n    try:\n        response = session.post(url, data=bulk_payload.encode('utf-8'), headers=headers, timeout=120)\n        \n        # Manejar errores espec√≠ficos\n        if response.status_code == 429:\n            wait_time = min(2 ** retry_count, 10)\n            print(f\"      ‚è≥ Elasticsearch sobrecargado, esperando {wait_time}s...\")\n            time.sleep(wait_time)\n            return bulk_index_documents(session, index_name, documents, retry_count + 1)\n        \n        if response.status_code == 413:\n            if len(documents) \u003e 1:\n                print(f\"      ‚ö† Payload muy grande, dividiendo lote...\")\n                mid = len(documents) // 2\n                s1, f1 = bulk_index_documents(session, index_name, documents[:mid], 0)\n                s2, f2 = bulk_index_documents(session, index_name, documents[mid:], 0)\n                return s1 + s2, f1 + f2\n            else:\n                print(f\"      ‚úó Documento demasiado grande, saltando\")\n                return 0, 1\n        \n        response.raise_for_status()\n        result = response.json()\n        \n        # Contar √©xitos y fallos\n        success = 0\n        failed = 0\n        errors = []\n        \n        if 'items' in result:\n            for item in result['items']:\n                if 'index' in item:\n                    if item['index'].get('status') in [200, 201]:\n                        success += 1\n                    else:\n                        failed += 1\n                        if 'error' in item['index']:\n                            error_type = item['index']['error'].get('type', 'unknown')\n                            if error_type not in errors:\n                                errors.append(error_type)\n        \n        if errors and failed \u003e 0:\n            print(f\"      ‚ö† Tipos de errores: {', '.join(errors[:3])}\")\n        \n        return success, failed\n        \n    except requests.exceptions.Timeout:\n        print(f\"      ‚è≥ Timeout, reintentando...\")\n        time.sleep(2)\n        return bulk_index_documents(session, index_name, documents, retry_count + 1)\n    except Exception as e:\n        print(f\"      ‚úó Error en bulk index: {e}\")\n        if retry_count \u003c 3:\n            time.sleep(2)\n            return bulk_index_documents(session, index_name, documents, retry_count + 1)\n        return 0, len(documents)\n\ndef sync_collection(session, mongo_client, db_name, collection_name):\n    \"\"\"Sincroniza una colecci√≥n de MongoDB a Elasticsearch\"\"\"\n    try:\n        collection = mongo_client[db_name][collection_name]\n        doc_count = collection.count_documents({})\n        \n        if doc_count == 0:\n            print(f\"   ‚äò Colecci√≥n vac√≠a, saltando\")\n            return 0\n        \n        # Limitar documentos si es necesario\n        docs_to_process = min(doc_count, MAX_DOCS_PER_COLLECTION)\n        if docs_to_process \u003c doc_count:\n            print(f\"   ‚ö† Limitando a {docs_to_process} de {doc_count} documentos\")\n        \n        print(f\"   ‚Üí Sincronizando {docs_to_process} documentos en lotes de {BATCH_SIZE}...\")\n        \n        # Indexar en Elasticsearch\n        index_name = get_index_name(db_name, collection_name)\n        total_success = 0\n        total_failed = 0\n        total_skipped = 0\n        \n        # Procesar en lotes\n        batch = []\n        batch_count = 0\n        processed = 0\n        \n        # Usar cursor con batch_size\n        cursor = collection.find().limit(docs_to_process).batch_size(BATCH_SIZE)\n        \n        for doc in cursor:\n            converted = convert_doc(doc, db_name, collection_name)\n            if converted:\n                batch.append(converted)\n            else:\n                total_skipped += 1\n            \n            # Cuando el lote est√° lleno, indexar\n            if len(batch) \u003e= BATCH_SIZE:\n                success, failed = bulk_index_documents(session, index_name, batch)\n                total_success += success\n                total_failed += failed\n                batch = []  # Limpiar\n                batch_count += 1\n                processed += BATCH_SIZE\n                \n                # Peque√±o delay cada 5 lotes\n                if batch_count % 5 == 0:\n                    time.sleep(1)\n                    print(f\"      ... {processed}/{docs_to_process} documentos procesados\")\n        \n        # Cerrar cursor\n        cursor.close()\n        \n        # Indexar el √∫ltimo lote\n        if batch:\n            success, failed = bulk_index_documents(session, index_name, batch)\n            total_success += success\n            total_failed += failed\n            batch = []\n        \n        print(f\"   ‚úì Sincronizados {total_success} documentos al √≠ndice '{index_name}'\")\n        \n        if total_failed \u003e 0:\n            print(f\"   ‚ö† {total_failed} documentos fallaron\")\n        \n        if total_skipped \u003e 0:\n            print(f\"   ‚ö† {total_skipped} documentos saltados por tama√±o\")\n        \n        return total_success\n        \n    except Exception as e:\n        print(f\"   ‚úó Error al sincronizar colecci√≥n: {e}\")\n        import traceback\n        traceback.print_exc()\n        return 0\n\ndef main():\n    \"\"\"Funci√≥n principal\"\"\"\n    print_header()\n    \n    # Crear sesi√≥n HTTP\n    session = create_session()\n    \n    # Conectar a MongoDB y Elasticsearch\n    mongo_client = connect_mongodb()\n    test_elasticsearch(session)\n    \n    print()\n    \n    # Obtener lista de bases de datos\n    try:\n        print(\"‚Üí Obteniendo lista de bases de datos...\")\n        db_list = [db for db in mongo_client.list_database_names() if db not in SYSTEM_DBS]\n        print(f\"‚úì Encontradas {len(db_list)} bases de datos\\n\")\n    except Exception as e:\n        print(f\"‚úó Error al obtener lista de bases de datos: {e}\")\n        sys.exit(1)\n    \n    # Estad√≠sticas\n    total_dbs = 0\n    total_collections = 0\n    total_documents = 0\n    \n    # Sincronizar cada base de datos\n    for db_name in db_list:\n        try:\n            db = mongo_client[db_name]\n            collections = db.list_collection_names()\n            \n            if not collections:\n                continue\n            \n            print(f\"üìÅ Base de datos: {db_name}\")\n            print(f\"   Colecciones: {len(collections)}\\n\")\n            \n            total_dbs += 1\n            \n            for collection_name in collections:\n                print(f\"   üìÑ Colecci√≥n: {collection_name}\")\n                docs_synced = sync_collection(session, mongo_client, db_name, collection_name)\n                total_documents += docs_synced\n                total_collections += 1\n                print()\n                \n                # Peque√±o delay entre colecciones\n                time.sleep(0.5)\n            \n        except Exception as e:\n            print(f\"‚úó Error al procesar base de datos {db_name}: {e}\\n\")\n            continue\n    \n    # Cerrar conexiones\n    mongo_client.close()\n    session.close()\n    \n    # Resumen final\n    print(\"=\" * 60)\n    print(\"‚úì Sincronizaci√≥n completada\")\n    print(\"=\" * 60)\n    print(f\"  Bases de datos procesadas: {total_dbs}\")\n    print(f\"  Colecciones procesadas: {total_collections}\")\n    print(f\"  Documentos sincronizados: {total_documents}\")\n    print(\"=\" * 60)\n\nif __name__ == \"__main__\":\n    main()\n"
            },
            "kind": "ConfigMap",
            "metadata": {
                "annotations": {
                    "kubectl.kubernetes.io/last-applied-configuration": "{\"apiVersion\":\"v1\",\"data\":{\"sync.py\":\"#!/usr/bin/env python3\\n# -*- coding: utf-8 -*-\\n\\\"\\\"\\\"\\nScript de sincronizaci√≥n MongoDB ‚Üí Elasticsearch\\nVersi√≥n mejorada con l√≠mite de tama√±o de documentos y manejo robusto de errores\\n\\\"\\\"\\\"\\n\\nimport os\\nimport sys\\nimport json\\nimport time\\nfrom datetime import datetime\\nfrom decimal import Decimal\\nfrom pymongo import MongoClient\\nfrom bson import ObjectId, Decimal128\\nimport requests\\nfrom requests.adapters import HTTPAdapter\\nfrom urllib3.util.retry import Retry\\nfrom urllib.parse import quote_plus\\n\\n# Configuraci√≥n desde variables de entorno\\nMONGO_HOST = os.getenv('MONGO_HOST', 'mongo.mongo.svc.cluster.local')\\nMONGO_PORT = int(os.getenv('MONGO_PORT', '27017'))\\nMONGO_USERNAME = os.getenv('MONGO_USERNAME', 'root')\\nMONGO_PASSWORD = os.getenv('MONGO_PASSWORD', '')\\nMONGO_AUTH_DB = os.getenv('MONGO_AUTH_DB', 'admin')\\n\\nES_HOST = os.getenv('ELASTICSEARCH_HOST', 'elasticsearch.elk.svc.cluster.local')\\nES_PORT = int(os.getenv('ELASTICSEARCH_PORT', '9200'))\\nES_URL = f'http://{ES_HOST}:{ES_PORT}'\\n\\nBATCH_SIZE = int(os.getenv('BATCH_SIZE', '50'))\\nMAX_DOCS_PER_COLLECTION = int(os.getenv('MAX_DOCS_PER_COLLECTION', '10000'))\\n\\n# NUEVO: L√≠mites de tama√±o optimizados\\nMAX_DOC_SIZE_BYTES = int(os.getenv('MAX_DOC_SIZE_BYTES', '524288'))  # 512KB por documento\\nCONTENT_FIELD_MAX_SIZE = int(os.getenv('CONTENT_FIELD_MAX_SIZE', '102400'))  # 100KB para campos de texto\\nLARGE_FIELD_THRESHOLD = int(os.getenv('LARGE_FIELD_THRESHOLD', '51200'))  # 50KB umbral\\n\\n# Bases de datos del sistema que no se sincronizan\\nSYSTEM_DBS = ['admin', 'config', 'local']\\n\\ndef print_header():\\n    \\\"\\\"\\\"Imprime el encabezado del script\\\"\\\"\\\"\\n    print(\\\"=\\\" * 60)\\n    print(\\\"MongoDB ‚Üí Elasticsearch Sync (Mejorado v2)\\\")\\n    print(\\\"=\\\" * 60)\\n    print(f\\\"MongoDB: {MONGO_HOST}:{MONGO_PORT}\\\")\\n    print(f\\\"Elasticsearch: {ES_URL}\\\")\\n    print(f\\\"Batch size: {BATCH_SIZE}\\\")\\n    print(f\\\"Max docs/collection: {MAX_DOCS_PER_COLLECTION}\\\")\\n    print(f\\\"Max doc size: {MAX_DOC_SIZE_BYTES / 1024:.0f}KB\\\")\\n    print(\\\"=\\\" * 60)\\n    print()\\n\\ndef create_session():\\n    \\\"\\\"\\\"Crea una sesi√≥n de requests con retry autom√°tico y pool de conexiones limitado\\\"\\\"\\\"\\n    session = requests.Session()\\n    \\n    # Configurar retry autom√°tico\\n    retry_strategy = Retry(\\n        total=3,\\n        backoff_factor=1,\\n        status_forcelist=[429, 500, 502, 503, 504],\\n    )\\n    \\n    adapter = HTTPAdapter(\\n        max_retries=retry_strategy,\\n        pool_connections=5,\\n        pool_maxsize=10,\\n        pool_block=True\\n    )\\n    \\n    session.mount(\\\"http://\\\", adapter)\\n    session.mount(\\\"https://\\\", adapter)\\n    \\n    return session\\n\\ndef connect_mongodb():\\n    \\\"\\\"\\\"Conecta a MongoDB y retorna el cliente\\\"\\\"\\\"\\n    try:\\n        print(f\\\"‚Üí Conectando a MongoDB: {MONGO_HOST}:{MONGO_PORT}\\\")\\n        \\n        # Crear URI de conexi√≥n con URL encoding para username y password\\n        if MONGO_USERNAME and MONGO_PASSWORD:\\n            # URL encode username y password para manejar caracteres especiales\\n            username_encoded = quote_plus(MONGO_USERNAME)\\n            password_encoded = quote_plus(MONGO_PASSWORD)\\n            mongo_uri = f\\\"mongodb://{username_encoded}:{password_encoded}@{MONGO_HOST}:{MONGO_PORT}/{MONGO_AUTH_DB}?authSource={MONGO_AUTH_DB}\\\"\\n        else:\\n            mongo_uri = f\\\"mongodb://{MONGO_HOST}:{MONGO_PORT}/\\\"\\n        \\n        # Conectar con configuraci√≥n optimizada\\n        client = MongoClient(\\n            mongo_uri,\\n            serverSelectionTimeoutMS=10000,\\n            connectTimeoutMS=10000,\\n            maxPoolSize=10,\\n            minPoolSize=1,\\n            maxIdleTimeMS=30000\\n        )\\n        \\n        # Verificar conexi√≥n\\n        client.admin.command('ping')\\n        server_info = client.server_info()\\n        print(f\\\"‚úì Conectado a MongoDB {server_info['version']}\\\")\\n        \\n        return client\\n    except Exception as e:\\n        print(f\\\"‚úó Error al conectar a MongoDB: {e}\\\")\\n        sys.exit(1)\\n\\ndef test_elasticsearch(session):\\n    \\\"\\\"\\\"Verifica que Elasticsearch est√© accesible\\\"\\\"\\\"\\n    try:\\n        print(f\\\"‚Üí Conectando a Elasticsearch: {ES_URL}\\\")\\n        response = session.get(ES_URL, timeout=10)\\n        response.raise_for_status()\\n        info = response.json()\\n        print(f\\\"‚úì Conectado a Elasticsearch {info['version']['number']}\\\")\\n        return True\\n    except Exception as e:\\n        print(f\\\"‚úó Error al conectar a Elasticsearch: {e}\\\")\\n        print(f\\\"   URL intentada: {ES_URL}\\\")\\n        sys.exit(1)\\n\\ndef get_index_name(db_name, collection_name):\\n    \\\"\\\"\\\"Genera el nombre del √≠ndice en Elasticsearch\\\"\\\"\\\"\\n    date_suffix = datetime.now().strftime('%Y.%m.%d')\\n    index_name = f\\\"mongodb-{db_name}-{collection_name}-{date_suffix}\\\".lower()\\n    index_name = index_name.replace('_', '-')\\n    return index_name\\n\\ndef json_serializer(obj):\\n    \\\"\\\"\\\"Serializador personalizado para tipos no est√°ndar de JSON\\\"\\\"\\\"\\n    if isinstance(obj, ObjectId):\\n        return str(obj)\\n    elif isinstance(obj, datetime):\\n        return obj.isoformat()\\n    elif isinstance(obj, Decimal128):\\n        return float(obj.to_decimal())\\n    elif isinstance(obj, Decimal):\\n        return float(obj)\\n    elif isinstance(obj, bytes):\\n        try:\\n            return obj.decode('utf-8')\\n        except:\\n            return str(obj)\\n    else:\\n        return str(obj)\\n\\ndef truncate_large_fields(doc, max_size=MAX_DOC_SIZE_BYTES):\\n    \\\"\\\"\\\"Trunca campos grandes en el documento con indexaci√≥n selectiva y metadata\\\"\\\"\\\"\\n    try:\\n        # Inicializar metadata de truncado\\n        truncation_metadata = {\\n            'fields_truncated': [],\\n            'original_sizes': {},\\n            'total_truncated': False\\n        }\\n        \\n        # Convertir a JSON para medir tama√±o\\n        json_str = json.dumps(doc, default=json_serializer, ensure_ascii=False)\\n        current_size = len(json_str.encode('utf-8'))\\n        \\n        # Si est√° dentro del l√≠mite, no hacer nada\\n        if current_size \\u003c= max_size:\\n            return doc, False\\n        \\n        # Identificar campos grandes\\n        large_fields = []\\n        for key, value in doc.items():\\n            if key == '_id':\\n                continue\\n            try:\\n                field_str = json.dumps(value, default=json_serializer, ensure_ascii=False)\\n                field_size = len(field_str.encode('utf-8'))\\n                if field_size \\u003e LARGE_FIELD_THRESHOLD:  # 50KB umbral\\n                    large_fields.append((key, field_size, value))\\n            except:\\n                continue\\n        \\n        # Truncar campos grandes con estrategia selectiva\\n        truncated = False\\n        for field_name, field_size, original_value in sorted(large_fields, key=lambda x: x[1], reverse=True):\\n            # Guardar tama√±o original\\n            truncation_metadata['original_sizes'][field_name] = field_size\\n            \\n            if isinstance(doc[field_name], str):\\n                # Truncar strings largos a 100KB\\n                max_chars = CONTENT_FIELD_MAX_SIZE // 4  # ~25K caracteres para UTF-8\\n                if len(doc[field_name]) \\u003e max_chars:\\n                    doc[field_name] = doc[field_name][:max_chars] + \\\"\\\\n\\\\n[... CONTENT TRUNCATED ...]\\\\n\\\\nOriginal size: \\\" + str(field_size) + \\\" bytes\\\"\\n                    truncated = True\\n                    truncation_metadata['fields_truncated'].append(field_name)\\n                    \\n            elif isinstance(doc[field_name], list):\\n                # Para listas, intentar reducir elementos\\n                if field_size \\u003e CONTENT_FIELD_MAX_SIZE:\\n                    # Calcular cu√°ntos elementos mantener\\n                    avg_item_size = field_size / len(doc[field_name]) if len(doc[field_name]) \\u003e 0 else field_size\\n                    keep_items = max(1, int(CONTENT_FIELD_MAX_SIZE / avg_item_size))\\n                    doc[field_name] = doc[field_name][:keep_items]\\n                    doc[field_name].append({\\n                        \\\"_truncation_info\\\": f\\\"List truncated from {len(original_value)} to {keep_items} items\\\",\\n                        \\\"original_size_bytes\\\": field_size\\n                    })\\n                    truncated = True\\n                    truncation_metadata['fields_truncated'].append(field_name)\\n                    \\n            elif isinstance(doc[field_name], dict):\\n                # Para diccionarios grandes, mantener estructura pero truncar valores\\n                if field_size \\u003e CONTENT_FIELD_MAX_SIZE:\\n                    # Convertir a string resumido\\n                    doc[field_name] = {\\n                        \\\"_truncation_info\\\": \\\"Large object truncated\\\",\\n                        \\\"original_size_bytes\\\": field_size,\\n                        \\\"keys_count\\\": len(doc[field_name]),\\n                        \\\"sample_keys\\\": list(doc[field_name].keys())[:10]\\n                    }\\n                    truncated = True\\n                    truncation_metadata['fields_truncated'].append(field_name)\\n            \\n            # Verificar si ya est√° dentro del l√≠mite\\n            json_str = json.dumps(doc, default=json_serializer, ensure_ascii=False)\\n            current_size = len(json_str.encode('utf-8'))\\n            if current_size \\u003c= max_size:\\n                break\\n        \\n        # A√±adir metadata de truncado al documento\\n        if truncated:\\n            truncation_metadata['total_truncated'] = True\\n            doc['_truncation_metadata'] = truncation_metadata\\n        \\n        return doc, truncated\\n    except Exception as e:\\n        print(f\\\"      ‚ö† Error al truncar documento: {e}\\\")\\n        return doc, False\\n\\ndef convert_doc(doc, db_name=None, collection_name=None):\\n    \\\"\\\"\\\"Convierte un documento de MongoDB a formato JSON serializable\\\"\\\"\\\"\\n    try:\\n        # Convertir ObjectId a string\\n        if '_id' in doc:\\n            doc['_id'] = str(doc['_id'])\\n        \\n        # A√±adir timestamp de indexaci√≥n\\n        doc['@timestamp'] = datetime.utcnow().isoformat() + 'Z'\\n        \\n        # A√±adir metadata de origen\\n        if db_name:\\n            doc['mongodb_database'] = db_name\\n        if collection_name:\\n            doc['mongodb_collection'] = collection_name\\n        if db_name and collection_name:\\n            doc['mongodb_namespace'] = f\\\"{db_name}.{collection_name}\\\"\\n        \\n        # Verificar y truncar si es necesario\\n        doc, was_truncated = truncate_large_fields(doc)\\n        \\n        if was_truncated:\\n            print(f\\\"      ‚ö† Documento {doc['_id']} truncado por tama√±o\\\")\\n        \\n        # Intentar serializar para detectar problemas\\n        json.dumps(doc, default=json_serializer)\\n        return doc\\n    except Exception as e:\\n        print(f\\\"      ‚ö† Error al convertir documento: {e}\\\")\\n        return None\\n\\ndef bulk_index_documents(session, index_name, documents, retry_count=0):\\n    \\\"\\\"\\\"Indexa documentos en Elasticsearch usando Bulk API con retry autom√°tico\\\"\\\"\\\"\\n    if not documents:\\n        return 0, 0\\n    \\n    # L√≠mite de reintentos\\n    if retry_count \\u003e 3:\\n        print(f\\\"      ‚úó M√°ximo de reintentos alcanzado\\\")\\n        return 0, len(documents)\\n    \\n    # Preparar el payload para Bulk API\\n    lines = []\\n    for doc in documents:\\n        if doc is None:\\n            continue\\n        \\n        try:\\n            # Extraer el _id y eliminarlo del documento\\n            doc_id = doc.get('_id', '')\\n            \\n            # Crear una copia del documento SIN el campo _id\\n            doc_without_id = {k: v for k, v in doc.items() if k != '_id'}\\n            \\n            # Acci√≥n de indexaci√≥n\\n            action = {\\\"index\\\": {\\\"_index\\\": index_name, \\\"_id\\\": doc_id}}\\n            action_line = json.dumps(action, default=json_serializer)\\n            \\n            # Documento\\n            doc_line = json.dumps(doc_without_id, default=json_serializer, ensure_ascii=False)\\n            \\n            lines.append(action_line)\\n            lines.append(doc_line)\\n        except Exception as e:\\n            print(f\\\"      ‚ö† Error al serializar documento {doc.get('_id', 'unknown')}: {e}\\\")\\n            continue\\n    \\n    if not lines:\\n        return 0, 0\\n    \\n    # Unir con newlines\\n    bulk_payload = '\\\\n'.join(lines) + '\\\\n'\\n    \\n    # Enviar a Elasticsearch\\n    url = f'{ES_URL}/_bulk'\\n    headers = {'Content-Type': 'application/x-ndjson'}\\n    \\n    try:\\n        response = session.post(url, data=bulk_payload.encode('utf-8'), headers=headers, timeout=120)\\n        \\n        # Manejar errores espec√≠ficos\\n        if response.status_code == 429:\\n            wait_time = min(2 ** retry_count, 10)\\n            print(f\\\"      ‚è≥ Elasticsearch sobrecargado, esperando {wait_time}s...\\\")\\n            time.sleep(wait_time)\\n            return bulk_index_documents(session, index_name, documents, retry_count + 1)\\n        \\n        if response.status_code == 413:\\n            if len(documents) \\u003e 1:\\n                print(f\\\"      ‚ö† Payload muy grande, dividiendo lote...\\\")\\n                mid = len(documents) // 2\\n                s1, f1 = bulk_index_documents(session, index_name, documents[:mid], 0)\\n                s2, f2 = bulk_index_documents(session, index_name, documents[mid:], 0)\\n                return s1 + s2, f1 + f2\\n            else:\\n                print(f\\\"      ‚úó Documento demasiado grande, saltando\\\")\\n                return 0, 1\\n        \\n        response.raise_for_status()\\n        result = response.json()\\n        \\n        # Contar √©xitos y fallos\\n        success = 0\\n        failed = 0\\n        errors = []\\n        \\n        if 'items' in result:\\n            for item in result['items']:\\n                if 'index' in item:\\n                    if item['index'].get('status') in [200, 201]:\\n                        success += 1\\n                    else:\\n                        failed += 1\\n                        if 'error' in item['index']:\\n                            error_type = item['index']['error'].get('type', 'unknown')\\n                            if error_type not in errors:\\n                                errors.append(error_type)\\n        \\n        if errors and failed \\u003e 0:\\n            print(f\\\"      ‚ö† Tipos de errores: {', '.join(errors[:3])}\\\")\\n        \\n        return success, failed\\n        \\n    except requests.exceptions.Timeout:\\n        print(f\\\"      ‚è≥ Timeout, reintentando...\\\")\\n        time.sleep(2)\\n        return bulk_index_documents(session, index_name, documents, retry_count + 1)\\n    except Exception as e:\\n        print(f\\\"      ‚úó Error en bulk index: {e}\\\")\\n        if retry_count \\u003c 3:\\n            time.sleep(2)\\n            return bulk_index_documents(session, index_name, documents, retry_count + 1)\\n        return 0, len(documents)\\n\\ndef sync_collection(session, mongo_client, db_name, collection_name):\\n    \\\"\\\"\\\"Sincroniza una colecci√≥n de MongoDB a Elasticsearch\\\"\\\"\\\"\\n    try:\\n        collection = mongo_client[db_name][collection_name]\\n        doc_count = collection.count_documents({})\\n        \\n        if doc_count == 0:\\n            print(f\\\"   ‚äò Colecci√≥n vac√≠a, saltando\\\")\\n            return 0\\n        \\n        # Limitar documentos si es necesario\\n        docs_to_process = min(doc_count, MAX_DOCS_PER_COLLECTION)\\n        if docs_to_process \\u003c doc_count:\\n            print(f\\\"   ‚ö† Limitando a {docs_to_process} de {doc_count} documentos\\\")\\n        \\n        print(f\\\"   ‚Üí Sincronizando {docs_to_process} documentos en lotes de {BATCH_SIZE}...\\\")\\n        \\n        # Indexar en Elasticsearch\\n        index_name = get_index_name(db_name, collection_name)\\n        total_success = 0\\n        total_failed = 0\\n        total_skipped = 0\\n        \\n        # Procesar en lotes\\n        batch = []\\n        batch_count = 0\\n        processed = 0\\n        \\n        # Usar cursor con batch_size\\n        cursor = collection.find().limit(docs_to_process).batch_size(BATCH_SIZE)\\n        \\n        for doc in cursor:\\n            converted = convert_doc(doc, db_name, collection_name)\\n            if converted:\\n                batch.append(converted)\\n            else:\\n                total_skipped += 1\\n            \\n            # Cuando el lote est√° lleno, indexar\\n            if len(batch) \\u003e= BATCH_SIZE:\\n                success, failed = bulk_index_documents(session, index_name, batch)\\n                total_success += success\\n                total_failed += failed\\n                batch = []  # Limpiar\\n                batch_count += 1\\n                processed += BATCH_SIZE\\n                \\n                # Peque√±o delay cada 5 lotes\\n                if batch_count % 5 == 0:\\n                    time.sleep(1)\\n                    print(f\\\"      ... {processed}/{docs_to_process} documentos procesados\\\")\\n        \\n        # Cerrar cursor\\n        cursor.close()\\n        \\n        # Indexar el √∫ltimo lote\\n        if batch:\\n            success, failed = bulk_index_documents(session, index_name, batch)\\n            total_success += success\\n            total_failed += failed\\n            batch = []\\n        \\n        print(f\\\"   ‚úì Sincronizados {total_success} documentos al √≠ndice '{index_name}'\\\")\\n        \\n        if total_failed \\u003e 0:\\n            print(f\\\"   ‚ö† {total_failed} documentos fallaron\\\")\\n        \\n        if total_skipped \\u003e 0:\\n            print(f\\\"   ‚ö† {total_skipped} documentos saltados por tama√±o\\\")\\n        \\n        return total_success\\n        \\n    except Exception as e:\\n        print(f\\\"   ‚úó Error al sincronizar colecci√≥n: {e}\\\")\\n        import traceback\\n        traceback.print_exc()\\n        return 0\\n\\ndef main():\\n    \\\"\\\"\\\"Funci√≥n principal\\\"\\\"\\\"\\n    print_header()\\n    \\n    # Crear sesi√≥n HTTP\\n    session = create_session()\\n    \\n    # Conectar a MongoDB y Elasticsearch\\n    mongo_client = connect_mongodb()\\n    test_elasticsearch(session)\\n    \\n    print()\\n    \\n    # Obtener lista de bases de datos\\n    try:\\n        print(\\\"‚Üí Obteniendo lista de bases de datos...\\\")\\n        db_list = [db for db in mongo_client.list_database_names() if db not in SYSTEM_DBS]\\n        print(f\\\"‚úì Encontradas {len(db_list)} bases de datos\\\\n\\\")\\n    except Exception as e:\\n        print(f\\\"‚úó Error al obtener lista de bases de datos: {e}\\\")\\n        sys.exit(1)\\n    \\n    # Estad√≠sticas\\n    total_dbs = 0\\n    total_collections = 0\\n    total_documents = 0\\n    \\n    # Sincronizar cada base de datos\\n    for db_name in db_list:\\n        try:\\n            db = mongo_client[db_name]\\n            collections = db.list_collection_names()\\n            \\n            if not collections:\\n                continue\\n            \\n            print(f\\\"üìÅ Base de datos: {db_name}\\\")\\n            print(f\\\"   Colecciones: {len(collections)}\\\\n\\\")\\n            \\n            total_dbs += 1\\n            \\n            for collection_name in collections:\\n                print(f\\\"   üìÑ Colecci√≥n: {collection_name}\\\")\\n                docs_synced = sync_collection(session, mongo_client, db_name, collection_name)\\n                total_documents += docs_synced\\n                total_collections += 1\\n                print()\\n                \\n                # Peque√±o delay entre colecciones\\n                time.sleep(0.5)\\n            \\n        except Exception as e:\\n            print(f\\\"‚úó Error al procesar base de datos {db_name}: {e}\\\\n\\\")\\n            continue\\n    \\n    # Cerrar conexiones\\n    mongo_client.close()\\n    session.close()\\n    \\n    # Resumen final\\n    print(\\\"=\\\" * 60)\\n    print(\\\"‚úì Sincronizaci√≥n completada\\\")\\n    print(\\\"=\\\" * 60)\\n    print(f\\\"  Bases de datos procesadas: {total_dbs}\\\")\\n    print(f\\\"  Colecciones procesadas: {total_collections}\\\")\\n    print(f\\\"  Documentos sincronizados: {total_documents}\\\")\\n    print(\\\"=\\\" * 60)\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n\"},\"kind\":\"ConfigMap\",\"metadata\":{\"annotations\":{},\"name\":\"mongodb-sync-script\",\"namespace\":\"elk\"}}\n"
                },
                "creationTimestamp": "2026-02-13T08:37:43Z",
                "name": "mongodb-sync-script",
                "namespace": "elk",
                "resourceVersion": "10770655",
                "uid": "e46ee6db-17b5-4a16-8924-2b67f779a15a"
            }
        }
    ],
    "kind": "List",
    "metadata": {
        "resourceVersion": ""
    }
}
